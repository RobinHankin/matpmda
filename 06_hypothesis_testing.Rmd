# Hypothesis Testing

As we discussed earlier, we often want to use a sample to make
inferences about the population from which the sample was drawn.  The
following notation is commonly used when referring to samples and
populations.  Here, we discuss how we can make inferences about a
population by testing a hypothesis that we may have regarding a
population parameter, such as a mean $\mu$ or proportion $\pi$.  We
will begin with an overview of the hypothesis testing and then will
explore this in more detail in remainder of this chapter.

Hypothesis testing is best understood through an example.  Suppose you
are given a coin and are wondering if the coin is biased towards
heads, that is, the probability of the coin landing heads is greater
than one half.  Mathematically we would write $p>0.5$, where $p$ is
the probability of the coin landing heads.  You toss the coin 100
times and find that it lands heads 60 times.  Is this evidence for the
coin being biased? 

The way to proceed is to think about what happens if the coin is
actually fair.  If the coin actually was fair, then~$p=\frac{1}{2}$.
We call the statement that the coin is fair the *Null Hypothesis*.  We
write $H_0$ or sometimes $H_\mathrm{NULL}$ for short.  In speech
sometimes we abbreviate to "the null".  The null hypothesis is always
the *absence* of the effect we are investigating.  In this case, we
are investigating whether the coin is biased towards heads, so the
null is that the coin is not biased, and is fair.  In mathematical
notation we would say

\[
H_0\colon\operatorname{Prob}(H)=\frac{1}{2}
\]

Since we are interested in whether the coin is biased towards heads, the alternative hypothesis is:

\[
H_1\colon\operatorname{Prob}(H) > \frac{1}{2}
\]


But the null does not mean that 100 tosses results in exactly 50
heads.  We can use R to conduct a computer simulation of this
situation.  Suppose we give 90 undergraduates a coin and tell them to
toss it 100 times and write down the number of heads.  In R this is
easy to simulate:

```{r}
set.seed(0)
rbinom(90,100,0.5)
```

In the above simulations, the probability of heads is exactly 0.5 (the
null hypothesis is true).  But the simulation shows that we actually
observe a range of heads.  Remember that originally we actually
observed 60 heads; this is our data.  If the null is true, we get 60
or 61 heads out of 100 tosses from time to time, so logically our
observation of 60 is not inconsistent with the null.

Note that it is mathematically *possible* for $p=0.5$ but nevertheless
observe 100 heads out of 100 tosses.  But if we did observe 100 heads,
we would conclude that the probability of a head was considerably more
than 0.5.

The way to think about it is to ask: If $p$ was actually equal to 0.5,
what is the probability that we observe 60 or more heads out of 100
tosses?  This is easily answered:
  
```{r}
sum(dbinom(60:100,100,0.5))
```

So, if the null is true the probability of observing~$x=60$ heads or
more is about 2.8\%, which is quite a small probability.  The fact
that this probability is small indicates that there is evidence
against the null being true.  The value of 2.8\% is known as a
*p-value*.  The p-value is the probability, if the null is true, of
obtaining our observation or one more extreme.  In this case, the
observation is 60 heads out of 100 tosses, and "more extreme" means
"more than 60 heads".

Note that *small* p-values constitute *strong* evidence against the
null; the smaller the p-value, the stronger the evidence against
$H_0$.  Conventionally, if the p-value is less than 5\% or 0.05 we say
that we have *statistically significant* evidence against the null.
In other words, if $p<0.05$ we *reject* the null and conclude that the
probability of the coin landing heads is greater than 0.5.

Hypothesis testing using p-values is a standard procedure with
standard steps.  Here is a summary of the coin example:
  
* Formulate a null hypothesis is that the coin is fair; write  $H_0\colon p=0.5$.
* Collect some data (in this case 60 heads out of 100 tosses)
* Calculate the p-value
* Compare the p-value to the standard value of 0.05
* If the p-value is less than 0.05, *reject* the null hypothesis
* If the p-value is not less than 0.05, make no conclusion.


Note that the opposite of "reject" is not "accept": in statistics, we
*never* accept a null hypothesis.  In this case, we can be sure that
$p$ is not *exactly* equal to 0.5.



## Notes about p-value

The Null hypothesis is a statement about the nature of a
population.  It is often stated in terms of a population parameter such
as the mean or standard deviation of a Gaussian distribution.

The alternative hypothesis is denoted by $H_1$ or $H_A$. The null
hypothesis will be rejected if it appears to be inconsistent with the
sample data and will not be rejected otherwise.

It is often a sound strategy to memorize the definition of p-value:

The p-value is the probability, if the null hypothesis is true, of
obtaining the observation or an observation more extreme.

What "more extreme" means depends on the situation, the null, and the
alternative hypothesis $H_1$.


## Critical region and test statistics

One useful concept is that of *test statistic*.  A test statistic is
an observable quantity (the best example is "sample mean") that has a
known distribution under the null.

An alternative approach to assessing whether or not our data are
consistent with the null hypothesis is to use a *critical region*.
The critical region, also called the "rejection region", is that set
of values of a test statistic for which we would reject the null.  If
the test statistic is in the critical region, then we reject $H_0$.

Figure \@ref(fig:criticalregion) shows the concept diagramatically.
We can see that, if the null is true, the probability of the test
statistic falling in to the rejection region is 0.05, or 5\%.

```{r criticalregion,fig.cap="The critical region shown in red", echo=FALSE,message=FALSE,warning=FALSE}
x <- seq(from=-4,to=4,len=100)
plot(x,dnorm(x),type="n",main="The critical region",xlab="sample mean",ylab="null")
xx <- seq(from=qnorm(0.95),to=4,len=100)
polygon(c(xx,rev(xx)),c(dnorm(xx),xx*0),col="red",border="blue")
points(x,dnorm(x),type="l",lwd=1)
legend("topleft",col=c("red","black"),lwd=c(11,1),legend=c("critical region","Null distribution"))
```


## One-sided vs two-sided p-values

Suppose we are testing a null that the population mean $\mu$ is a
particular value, $\mu_0$, so $H_0\colon \mu=\mu_0$.  We calculate the
sample mean~$\overline{x}$ and compare this with~$\mu_0$.  The idea is that
there are three types of change in a mean value that we might be
investigating:

* The population mean is known to be *larger* than the $\mu_0$, and
this case the difference $\overline{x}-\mu_0$ will be *positive*.  We
wish to find out whether or not the increase is significant and write
$H_1\colon \mu > \mu_0$.

* The population mean is known to be *smaller* than the null, and
this case the difference $\overline{x}-\mu_0$ will be *negative*.  We
wish to find out whether or not the decrease is significant and write
$H_1\colon \mu < \mu_0$.

* The mean might be larger or smaller than the null, and in this case
we do not know whether $\overline{x}-\mu_0$ is positive or negative.
We are seeking evidence that the difference, whether an increase or
a decrease, is significant and write  $H_1\colon \mu \neq \mu_0$.

The first two cases are called *one-tailed tests* or *one-sided tests*
and the third is called a *two-tailed test*.  These are shown in
figures \@ref(fig:onesidedpvalue) and \@ref(fig:twosidedpvalue).  We will
see examples of both types of test soon.

```{r onesidedpvalue,fig.cap="pvalue for a one-sided test", echo=FALSE,message=FALSE,warning=FALSE}
x <- seq(from=-4,to=4,by=0.001)
plot(x, dnorm(x),type="l", ylab="Density", xlab="", xaxt="n",main="One-sided test")
jj <- x[x>qnorm(0.95)]
polygon(x=c(jj,rev(jj)), y=c(dnorm(jj),jj*0),col="red")
abline(v=qnorm(0.95))
text(qnorm(0.95),0.1,"observation",pos=4)
legend("topleft",col=c("red","black"),lwd=c(11,1),legend=c("p-value","Null distribution"))
```

```{r twosidedpvalue,fig.cap="pvalue for a two-sided test", echo=FALSE,message=FALSE,warning=FALSE}
x <- seq(from=-4,to=4,by=0.001)
plot(x, dnorm(x),type="l", ylab="Density", xlab="", xaxt="n",main="Two-sided test")
jj <- x[x>qnorm(0.975)]
polygon(x=c(jj,rev(jj)), y=c(dnorm(jj),jj*0),col="red")
jj <- -jj 
polygon(x=c(jj,rev(jj)), y=c(dnorm(jj),jj*0),col="red")
abline(v=qnorm(0.975))
text(qnorm(0.975),0.1,"observation",pos=4)
legend("topleft",col=c("red","black"),lwd=c(11,1),legend=c("pvalue","Null distribution"))
```

The above hypothesis tests are straightforward to apply using builtin
R functionality.

## The Z test

One way to consider the test statistic (here the sample mean) is to
convert it to a quantity, conventionally called $Z$, which has a
standard normal distribution if the null is true.  To do this, we need
to subtract the null mean, and scale by dividing by the standard
deviation:


\[
Z=\frac{\overline{x}-\mu}{\sigma}
\]

##  Example:  Passing with excellence in NCEA

Now suppose that we are interested in student pass rates at NCEA
level.  For a particular NCEA exam, we know that 20\% of New Zealand
students pass the exam with an "excellence" grade.  We are interested
in a particular school in which 150 students took the exam.  Of these
150 students, 21 pass with excellence.

*Question:* is there evidence for this school having a lower
probability of passing with excellence than the national New Zealand
rate?

* _Step 1_.  Realize that passing with excellence is a Bernoulli random
  variable, with success meaning "pass with excellence" and failure
  meaning any other result.
* _Step 2_. State a null hypothesis.  Here we would say that the null is
  that this school has a probability of passing with excellence of the
  national average, 20\% (or 0.2).  Mathematically we write $H_0\colon
  p=0.2$, where $p$ is the probability for students in this school
  passing with excellence.  The alternative hypothesis is $p<0.2$.
* _Step 3_.  Realize that the null hypothesis implies that the number of
  excellence students will be binomial with size 150 and probability
  0.2.  So the number of students with excellence will be
  $\sim\operatorname{Bin}(150,0.2)$.  The expected number of excellence
  students will be $150\times 0.2=30$.
* _Step 4_.  State the precise definition of *p-value*: "the
  probability, if the null is true, of obtaining the observation or an
  observation more extreme".  Here "more extreme" means that the
  number of excellence students is lower than the observed number, 21.
* _Step 5_. Use R to calculate the p value:

```{r}
sum(dbinom(0:21,150,0.2))
```

* _Step 6_.  Compare the pvalue of about 3.7\% with the 5\% standard
  value.  This is less than 5\% So we *reject* the null.
 * _Step 7_.  Interpret: we have strong evidence that this particular
  school has a *lower* probability of passing with NCEA excellence
  than the national average of 0.2


## Example: weights of kiwi birds.

An ecologist catches kiwi birds in a particular forest and weighs
them.  The data is as follows:

```{r}
kiwi <- 
c(1.08, 1, 0.96, 0.84, 0.91, 1.21, 1.03, 1.08, 1.09, 1.04, 0.95, 
0.87, 1.09, 1.09, 1.08, 0.93, 1.07, 1.16, 1.21, 0.94, 1.16, 0.92, 
1.09, 1.01, 1.08, 1.07, 0.89, 1.02, 1.14, 0.9)
```

Nationally, the weight of a kiwi bird is known to have a mean of 1kg.

*Question*: Is there any evidence for this set of kiwi to differ from
the national mean of 1kg?

* _*STEP*_ *1*. State a null hypothesis.  Here we would say that the null is
  that the population mean of our kiwi is that of the national average, 1kg.
   Mathematically we write $H_0\colon
  \mu=1.0$, where $\mu$ is the mean weight of kiwi in the forest in question.
  The alternative hypothesis is $\mu\neq 1.0$.
* _STEP 2_.  Figure out the sample mean (which is our test statistic), 
  and standard deviation:

```{r}
mean(kiwi)
sd(kiwi)
```

* *STEP 3*.  State the precise definition of *p-value*: "the
  probability, if the null is true, of obtaining the observation or an
  observation more extreme".  Here "more extreme" means "further away
  from 1kg" and we have a two-sided test, see figure
  \@ref(fig:twosidedpvalue).

* *STEP 4*.  Calculate the null distribution of the test statistic.
  This has mean 1kg and standard deviation equal to the standard error
  of the mean, $\frac{\sigma}{\sqrt{n}}$:

```{r}
SEM <- sd(kiwi)/sqrt(30)
SEM
```

* *STEP 5*.  Calculate the p-value.  We can see that the p-value has
  two pieces, and the area of one piece is just  `1-pnorm(mean(kiwi),mean=1,sd=SEM)`
  so the total p-value is twice this, or 

```{r}
2*(1-pnorm(mean(kiwi),mean=1,sd=SEM))
```

 * STEP 6.  Interpret: This p-value exceeds the 5\% requirement, so
   the result is *not significant*.  There is no evidence that the
   weights of these kiwi birds differ from the national average of
   1kg.

Alternatively we could transform our sample mean to give a Z-score:

```{r}
Z <- (mean(kiwi)-1)/SEM
2*(1-pnorm(Z))
```

which gives the same result.  We will need the Z score approach when
we consider the Student t-test.