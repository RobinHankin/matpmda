# The Normal Distribution


In this chapter we are going to look at a continuous distribution
called the Normal, or Gaussian, distribution.  The Normal distribution
has many applications and occurs throughout science, medicine, and
commerce from astronomy to zoology.  Its importance and some of its
mathematical properties were discovered and investigated in the 18th
century by German mathematician Carl Friedrich Gauss.  The curve of
the Normal distribution is nice and smooth, and has a characteristic
shape sometimes referred to as a "bell curve".

Figure \@ref(fig:annotatednorm1) shows a generic normal distribution
showing the mean value as the maximum of the curve, and defining the
standard deviation to be the distance from the mean to the steepest
point.  The standard deviation is a measure of the "width" of a distribution.

```{r annotatednorm1,fig.cap="The normal (Gaussian) distribution showing mean and standard deviation.", echo=FALSE,message=FALSE,warning=FALSE}
x <- seq(from=-4,to=4,len=1000)
plot(x,dnorm(x),type="l",lwd=6,main="Gaussian distribution")
abline(v=0,lwd=3)
text(0,0.05,'mean',pos=4)
abline(v=1,lwd=1)
points(1,dnorm(1),cex=4)
text(1,dnorm(1),"  steepest point",pos=4)
arrows(0,0.15,1,0.15,code=3)
text(0.5,0.15,'standard',pos=3)
text(0.5,0.15,'deviation',pos=1)
```

Figure \@ref(fig:manynorm) shows some normal distributions with
varying mean values (the curve moves left and right) and widths (the
curve becomes wider or narrower).

```{r manynorm,fig.cap="A selection of normal (Gaussian) distributions with different means and standard deviations.  See how the mean value shifts the distribution to the left or the right, and the standard deviation makes the distribution wider or narrower", echo=FALSE,message=FALSE,warning=FALSE}
x <- seq(from = -4,to=4,len=100)
plot(x,dnorm(x),lwd=3,type='l',col='black',ylim=c(0,1))
grid()
points(x,dnorm(x,mean=1,sd=1),type='l',lwd=3,col='red')
points(x,dnorm(x,mean=1,sd=2),lwd=3,type='l',col='blue')
points(x,dnorm(x,mean=-2,sd=0.5),lwd=3,type='l',col='green')
legend("topright",lwd=3,col=c("black","red","blue","green"),
       legend=c("mean=0, sd=1","mean=1,sd=1","mean=1,sd=2","mean=-2,sd=0.5"))
```


## Properties of the Normal Distribution

Suppose $X$ is a Normally distributed random variable with mean $\mu$
and standard deviation $\sigma$.  Recall that the variance is the
square of the standard deviation; the standard deviation is the square
root of the variance.  We can write

\[X \sim N(\mu, \sigma)\]

* The expected value of $X$ is $E[X] = \mu$.
* The standard deviation of $X$ is $\sigma$.
* The variance of $X$ is $\sigma^2$.

(note that notation is sometimes ambiguous and the second argument
might refer to standard deviation or variance).

## The Density Function of the Normal Distribution

The *probability density function* (abbreviated "PDF") of the Normal
distribution is given by

\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{\left(x-\mu\right)^2}{2\sigma^2}\right]
\]

where

*  $x$ is the horizontal distance on the graph, the x-axis or _independent_ variable}
*  $\exp(\cdot)$ is the exponential function, sometimes written $e^x$.
*  $\pi=3.1416\ldots$ is the ratio of a circle's diameter to its radius
*  $\sigma$ is the standard deviation; $\sigma^2$ is the square of the standard deviation, or the variance
*  $\mu$ is the mean of the distribution

## The standard Normal Distribution

The standard Normal distribution has $\sigma=1$ and $\mu=0$ so the
density becomes


\[
\frac{1}{\sqrt{2\pi}}\exp\left[\frac{x^2}{2\sigma^2}\right].
\]

If $Z$ is distributed as a standard Normal random variable, then we
can write $Z \sim N(0,1)$.  Note that R can calculate this for you, as
discussed below.  From the formula you can deduce a number of facts
about the standard Normal distribution:
  

* It is symmetrical about $x=0$ because $x^2=(-x)^2$.
* It has a unique maximum at $x=0$ because $e^{-x}<1$ whenever $x>0$.
* It is infinitely wide because $e^x=0$ can never happen; the curve does not touch the x axis.


## Relationship between Normal and Standard Normal

Suppose $X \sim N(\mu, \sigma^2)$ and $Z \sim N(0,1)$.

The random variable $X$ can be converted to a standard Normal random
variable as follows:

\[
Z = \frac{X - \mu}{\sigma}
\]

Then we would have
\[
P(X \leq x) = P( Z \leq \frac{x - \mu}{\sigma})  = P( Z \leq z)
\]

In the "old days", before R, statisticians had to use a book of tables
to compute the probabilities. Rather than carrying around a table for
every single combination of $\mu$ and $\sigma$, they would use tables
for the standard Normal distribution and convert their $X$ to a $Z$ as
shown above.

 
## Probability and the Normal Distribution

Basic facts: 

* Area under the pdf $f(x)$ is equal to 1
* Symmetric:  area to left of mean $\mu$ = 0.5 = area to right of mean $\mu$ 
* To compute $P(X \leq x)$ find the area to the left of $x$.
* Because $X$ is a continuous random variable, $P(X=x)=0$.  That is, the probability of $X$ being exactly equal to any particular value is zero.

```{r annotatednorm2,fig.cap="The normal (Gaussian) distribution showing mean and standard deviation.", echo=FALSE,message=FALSE,warning=FALSE}
mu  <-  0
sigma <- 1
x = seq(from=-4, to=4, by=0.001)
plot(x, dnorm(x, mu, sigma), type="l", lwd=3, ylab="f(x)", yaxt="n", xaxt="n")
axis(side=1, at=-3:3, labels=c(  expression(mu ~-3*sigma),  expression(mu ~-2 *sigma),  expression(mu~-1 *sigma),  expression(mu),  expression(mu ~+1 *sigma),  expression(mu ~+2 *sigma),  expression(mu ~+3 *sigma) ))
Fx = pnorm(-4:4, mu, sigma)
cols = c("black", "lightblue", "mediumblue", "darkblue","darkblue", "mediumblue",  "lightblue","black")
colText = c("black", "black", "white", "white", "white", "white", "black", "black")

for(i in -4:4){
  j <- i+1
  k <- 5
  ijvals <- seq(i, j, 0.001)
  polygon(x=c(i,ijvals , j), y= c(0, dnorm(ijvals, 0, 1), 0), col=cols[i+k], border="white")
  text((i+0.5), 0.05, labels=round(diff(Fx)[i+k],digits=3), col=colText[i+k])
}
```

## Four R Functions for the Normal distribution

There are four functions in R that are useful for understanding the Normal distribution.  These are:

* `rnorm()` gives random samples from the Normal distribution.
* `dnorm()` gives the density of the Normal (that is, the vertical coordinate of the Bell curve)
* `pnorm()` gives the area to the right of the Normal
* `qnorm()` the quantile function (see below).

You can get help on these at the R prompt by typing `?dnorm`

###  Random samples from the Normal: `rnorm()`

The R system can furnish random variables easily.  You can get random
samples from a Normal distribution using `rnorm()`
  
```{r}
rnorm(10)    # standard Normal
```

The "10" means to generate 10 observations.  From the help page we can
see that the default values of mean and standard deviation are 0 and 1
respectively.  We call the default values the ``standard Normal'',
sometimes written $N(0,1)$, with the (0,1) meaning that the mean is
zero and the standard deviation is 1.  It is easy to change the mean
and standard deviation.  Suppose we wanted 20 samples from a Normal
distribution with a mean of 10 and a standard deviation of 0.03:

```{r}
rnorm(20,mean=10,sd=0.03)
```

In the figure above, we specified the mean and standard deviation
explicitly, but naming them is optional.  We could have said
`rnorm(20,10,0.03)` and obtained the same result.  Graphical plots
are always informative; see the figure below for an example.

```{r}
plot(rnorm(1000),main="1000 random normal observations")
```

In the figure above, most of the observations are between $-2$ and
$+2$ but there are occasional outliers.  It is possible to produce a
histogram using `hist()` as in the figure below:


```{r}
hist(rnorm(10000), col='lightblue',main="histogram of normal observations")
```

The figure above shows a histogram of random observations from the
standard normal distribution.  See how most of the observations are
between $-2$ and $+2$ but there are occasional outliers.

## Density: `dnorm()`

Function `dnorm(x)` gives the density of the standard Normal
distribution at `x`.  Thus

```{r}
dnorm(0)
dnorm(1)
dnorm(3)
dnorm(-3)
```

It is easier to see this by plotting the curve:

```{r}
x <- seq(from=-4,to=4,len=100)
plot(x,dnorm(x),main="Density function of the standard normal distribution")
```


## The cumulative distribution function, `pnorm()`
If $X\sim N(0,1)$, we quite often need to find the probability that
$X$ is less than a particular value, as in the figure below:

```{r}
x <- seq(from=-3,to=3,len=100)
plot(x,dnorm(x),type="l",main="The cumulative density function")
x <- x[x<1.3]
polygon(x=c(x,rev(x)),y=dnorm(c(x+1e100,rev(x))),border=NA,col="gray")
```

The figure above shows the probability that $X\leq 1.3$ is given by
the gray area.  To give a numerical value that $X\leq x$ we use the
`pnorm()` command in R:

```{r}
pnorm(1.3)
```

```{r,echo=FALSE}
set.seed(0)
```

so the probability is about 90\%.  It is always a good idea to verify
such results numerically by random sampling:

```{r}
table(rnorm(1e6) < 1.3)
```
Thus 903503 out of one million (`1e6`) samples are less than 1.3.

The four normal functions all take a `mean` and `sd` argument,
so we can specify these in our call.  Suppose we ask what the
probability that a $N(5,2.2)$ Gaussian random variable is less than 6.1:

```{r}
pnorm(6.1,mean=5,sd=2.2)
```

To get the probability that $X$ is *above* a particular value, we need
to take one minus the probability that is is below.  Again if $X\sim
N(5,2.2)$, suppose we seek the probability that $X$ is *greater* than
4.3:

```{r}
1-pnorm(4.3,mean=5,sd=2.2)
```

If we want the probability that a Gaussian random variable lies
*between* two values, we need to subtract one value from another.
Suppose we have $X\sim N(10,2.2)$ and we want the probability that $X$
lies between say 11.2 and 11.9:

```{r}
pnorm(11.9,10,2.2)-pnorm(11.2,10,2.2)
```

Verifying this numerically needs a little more work:

```{r}
x <- rnorm(1e6,10,2.2)
table((x>11.2) & (x<11.9))
```



## The quantile function `qnorm()`

The quantile function `qnorm()` inverts the cumulative
distribution function.  Suppose we have a standard normal variable and
wish to find the point $x$ which is exceeded with a probability of
5\%.  This would mean that $P(X\leq x) = 0.9$.  To find this point we
use `qnorm()`:

```{r}
qnorm(0.95)
```

Thus the probability that a standard normal variable $X$ is less than
about $1.64$ is is 95\%, and so the probability that $X>1.64$ is about
5\%.  It is important to be able to check such findings:

```{r}
pnorm(1.644854)
```

Or indeed numerically:

```{r}
table(rnorm(1e6) < qnorm(0.95))/1e6
```

showing close agreement with the expected 5\% / 95\% split.  Remember
that `qnorm()` takes `mean` and `sd` arguments which can
be used to override the default values of 0 and 1.  It is possible to
graph the quantile function:

```{r}
p <- seq(from=0.01,to=0.99,len=100)
plot(p,qnorm(p))
```


## The normal distribution and the Binomial Distribution

The Normal distribution is closely related to the Binomial
distribution (see previous week).  Consider again the Binomial
distribution, but with a large value of $n$, say $n=100$, and $p=0.4$.
This distribution is plotted in Figure \@ref(fig:binomialnormalplot)
from which we can see that this binomial distribution is very closely
approximated by a normal distribution.

```{r binomialnormalplot,fig.cap="A histogram showing the distribution of the bin(100,0.3)", echo=TRUE,message=FALSE,warning=FALSE}
plot(0:100,dbinom(0:100,100,0.3),type='h')
```

In general, the binomial distribution for large $n$ is closely
approximated by a normal distribution.



### Mean and standard deviation of the binomial distribution

if $X\sim\operatorname{Bin}(n,p)$ then the expected value of $X$ is
given by $E(X)=np$ and the standard deviation $\operatorname{sd}(X)$
is given by $\sqrt{np(1-p)}$.  Sometimes textbooks will write $q=1-p$
(that is, $q$ is the probability of failure) and the standard
deviation is then $\sqrt{npq}$.  The variance, being the square of the
standard deviation, is $npq$.  We can verify these statements
numerically.  Suppose we have $n=10$ and $p=0.3$.  Then
$E(X)=np=10\times 0.3=3$, and $\operatorname{sd}(X)=\sqrt{10\times
0.3\times 0.7}=1.45$.  To check these values we use `rbinom()`:


```{r}
mean(rbinom(1e6,10,0.3))
sd(rbinom(1e6,10,0.3))
```

showing close agreement.  The figure below shows the cumulative
distribution function for $\operatorname{Bin}(20,0.3)$ and that of
$N(6,2.05)$ on the same axes:

```{r}
x <- seq(from=-3,to=10,len=100)
plot(x,pnorm(x,6,sqrt(20*0.3*0.7)),type="l",ylab="cumulative probability")
points(x,pbinom(x,20,0.3),type="l",col="red")
legend("topleft",lty=1,col=c("red","black"),legend=c("binomial","normal"))
```

In the figure above, we can see how the normal and binomial differ:
the normal distribution is continuous but the binomial is discrete
(which is why the CDF has jumps).  But the two distributions are
broadly the same.  The agreement becomes closer if $n$ becomes larger.