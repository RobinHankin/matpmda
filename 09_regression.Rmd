# Linear regression
	
All methods we have looked at so far have a single variable.  Here we
consider methods for two variables.  If you have more than one
variable, you may find that they are related in some kind of way.
Consider the ```mtcars``` dataframe, provided with R.  Looking at the
help page (type ```?mtcars``` at the R prompt), we see that each row
is a type of car, and each column is a feature of that car such as
fuel efficiency (miles per gallon, ```mpg```), number of cylinders,
and so on.


```{r}
head(mtcars)
``` 

We might wonder if there is a relationship between fuel efficiency
(```mpg```) and the car's weight (```wt```).  The first step would be
to plot a scattergraph:

```{r}
plot(mpg~wt,data=mtcars,main="Efficiency as a function of car weight")
```

In the above, we use the tilde ("```~```") to mean "is a function of".
So the line reads "plot ```mpg``` as-a-function-of ```wt```, with data
from ```mtcars```".  We can see that high weight cars are associated
with low fuel efficiency.  We would like to describe this relationship
mathematically.

## Mathematical basis for linear regression

Any scattergraph consists of points on a plane together with an x-axis
and a y-axis.  Each point is at position $(x_i,y_i)$, where
$i=1,2,\ldots, n$ is the number of the observation.  We are trying to
predict $y$ as a function of $x$.  The fundamental assumption of
regression is:

\[
y_i = A + Bx_i+\epsilon_i 
\]

where

* $y_i$ is the vertical position of point $i$
* $x_i$ is the horizontal position of point $i$
* $A,B$ are the intercept and slope of the line, to be found
* $\epsilon_i$ are random errors.  We usually specify that $\epsilon_i\sim N(0,\lambda^2)$, so the errors are mean zero Gaussian

Finding $A$ and $B$ is the mathematical equivalent of "making the line
go through the points". The process is mathematically involved (it
requires matrix algebra) but fortunately it is possible to
estimate $A, B$  using the ```lm()``` function built in to R:


```{r}
lm(mpg~wt,data=mtcars)
```

thus the intercept is 37.285 and the slope -5.344; we have $y=A+Bx$
with $A=37.285$ and $B=-5.344$, or we could write $y=37.285-5.344x$
for the equation of the straight line.  The intercept is the value of
$y$ when $x=0$ (that is, where the line intercepts the $y$ axis), and
the slope is the increase in $y$ for a unit increase in $x$.  It is
easy to add the fitted straight line on to the scatterplot using the
```abline()``` function:

```{r}
plot(mpg~wt,data=mtcars,main="Efficiency as a function of car weight")
abline(lm(mpg~wt,data=mtcars))
```
The process of fitting a straight line through a scattergraph of
points is known as *linear regression*.  This is a very powerful
technique and has applications throughout science.

## Residuals

Sometimes we write $\hat{y}$ for the predicted value of $y$ at a
particular value of $x$ and thus $\hat{y}=A+Bx$.  The difference
between the observed value $y$ and the predicted value $\hat{y}$ is
called the *residual*.  We hope that the residuals are small; and if
they are, this means we have an accurate model.  The residuals are
shown in red on figure \@ref(fig:residuals).

```{r residuals,fig.cap="Residuals on a regression", echo=FALSE,message=FALSE,warning=FALSE}
plot(mpg~wt,data=mtcars,main="Efficiency as a function of car weight; residuals in red",ylim=c(7,35))
fit <- lm(mpg~wt,data=mtcars)
abline(fit)
for(i in seq_len(nrow(mtcars))){
    segments(x0=mtcars$wt[i],y0=fitted.values(fit)[i],y1=mtcars$mpg[i],col="red")
}
```


## Measures of strength of association

Consider the two figures below:

```{r twoplots,fig.cap="two plots with different strengths of association",echo=FALSE}
x <- 1:100
plot(x,x+rnorm(100)*7,ylab="y")
plot(x,x+rnorm(100)*30,ylab="y")
```

It is clear that the first scattergraph is "weaker" in some sense, and
the second is "stronger".  There are two ways to quantify this: the
$p$-value, and the correlation coefficient.

Recall the definition of $p$-value: "the probability, if the null is
true, of obtaining the observation or an observation more extreme".
In the case of linear regression, the null is that the true slope of
the regression line is zero.  Mathematically, $H_0\colon B=0$.  In
textbooks you will often see $H_0\colon\beta=0$, as Greek letters are
used for the intercept and slope.

The correlation coefficient $\rho$ is a measure of the smallness of
the residuals compared with the vertical extent of the line.  We would
have $\rho=0$ for no correlation, $\rho=1$ for perfect positive
correlation, $\rho=-1$ for perfect negative correlation, and
intermediate values for intermediate levels of correlation; see figure
\@ref(fig:fourplots).  Sometimes you will see the *square* of the
correlation, $\rho^2$, often written "$R$".

```{r fourplots,fig.cap="Different correlation coefficients",echo=FALSE}
x <- 1:100
y1 <- +x+rnorm(100)
y2 <- +x+rnorm(100)*37
y3 <- -x+rnorm(100)*37
y4 <- rnorm(100)
par(mfrow=c(2,2))
plot(x,y1,main=expression(paste(rho,"=0.99")))
plot(x,y2,main=expression(paste(rho, "=0.5")))
plot(x,y3,main=expression(paste(rho,"=-0.5")))
plot(x,y4,main=expression(paste(rho,   "=0")))
```

The $p$-value and correlation coefficient $\rho$ measure very
different things.  The $p4$-value is a measure of the amount of
information collected, and the strength of evidence against the null;
the correlation coefficient $\rho$ measures the strength of the
association between the variabfferent correlation coefficients.

## Finding the $p$-value in R

To find the $p$-value for a linear regression we need to use the
```summary()``` command:

```{r}
summary(lm(mpg~wt,data=mtcars))
```

We can see that the $p$-value is ```1.294e-10```, much smaller than
the required 5\%: this means that the regression slope is significant
and we can infer that the variables ```mpg``` and ```wt``` really are
correlated.  The adjusted R-squared of 0.7746 would indicate a
reasonably tight relationship.

Note that R provides a *two-sided* $p$-value and if we want a
one-sided test we would have to halve the value that R gives.

## Warnings

Partly because regression is such a powerful and useful technique, it
is easily abused.  Here we discuss some of the many pitfalls
surrounding regression.

## Correlation does not imply causation

Just because two variables have a significant linear regression does
not mean that the $x$ variable *causes* the $y$ variable to change.
There are several classic examples of this phenomenon:

* Ice-cream sales and drownings.  These two variables have a strong
  positive association: more ice-cream sales are associated with more
  drownings.  Here, the link is due to the weather: on hot days, more
  ice cream is sold, and also more people visit the beach so there are
  more drownings.
* Watching violent TV is associated with violent behaviour.  This can
  be interpreted to mean that watching violent TV *causes* violent
  behaviour.  But it could just as easily be the other way round:
  maybe violent people tend to watch violent TV shows.

These are just two examples and there are many many others.
Establishing a causative link between two variables can be very, very
difficult, especially in the social sciences.  Causality itself is a
very difficult and slippery concept, even in clear-cut disciplines
such as physics.  But note too that associations can be interesting
and worth studying, irrespective of their causality.

## Assumptions of linear regression

Figure \@ref(fig:LINE) shows some scattergraphs with distributions
that are problematic for simple linear regression.  A useful mnemonic
is LINE: the distribution has to be Linear, and the errors
Independent, Normally distributed, and Equal variance.

```{r LINE,fig.cap="Contraindications for linear regression.   Top left, non-linear response; top right, non-independent errors; lower left, non-Gaussian errors; lower right, non-equal variances", echo=FALSE}
n <- 60
x <- seq(from=0,to=1,len=n)
par(mfrow=c(2,2))
plot(x,x-x^2+rnorm(n)*0.06,pch=16,xlab="",ylab="",main="nonlinear")
plot(x, sin(x*10)+rnorm(n)*0.1,pch=16,xlab="",ylab="",main="non independent")
plot(x,x+(rbeta(n,0.1,0.1))*0.4,pch=16,xlab="",ylab="",main="non normal")
plot(x, x+rnorm(n,sd=(1-x)*3)*0.1,pch=16,xlab="",ylab="",main="non-equal variance")
```

## Multiple regression


It is possible to apply linear regression to more than one independent
variable.  The simplest such case is two independent variables, which
are plotted on the x and y axes, the dependent variable being plotted
on the z (vertical) axis; see figure \@ref(fig:plot3d), taken from the
```plot3d``` R package.  This which shows a visual representation of a
regression using the ```mtcars``` dataset we considered in week 1 in
which ```mpg``` is regressed against ```wt``` and ```disp```.

```{r, plot3d, fig.cap="Visual representation of multivariate linear regression, taken from the plot3D package.  Variable ```mpg``` is shown on the vertical axis; variables ```wt``` and ```disp``` on the two horizontal axes.  The regression itself is a plane, rather than a line", echo=FALSE}
library("plot3D")
# Following code taken from scatter.Rd in the plot3D package
with (mtcars, {

  # linear regression
   fit <- lm(mpg ~ wt + disp)

  # predict values on regular xy grid
   wt.pred <- seq(1.5, 5.5, length.out = 30)
   disp.pred <- seq(71, 472, length.out = 30)
   xy <- expand.grid(wt = wt.pred, 
                     disp = disp.pred)

   mpg.pred <- matrix (nrow = 30, ncol = 30, 
      data = predict(fit, newdata = data.frame(xy), 
      interval = "prediction"))

# fitted points for droplines to surface
   fitpoints <- predict(fit) 

   scatter3D(z = mpg, x = wt, y = disp, pch = 18, cex = 2, 
      theta = 20, phi = 20, ticktype = "detailed",
      xlab = "wt", ylab = "disp", zlab = "mpg",  
      surf = list(x = wt.pred, y = disp.pred, z = mpg.pred,  
                  facets = NA, fit = fitpoints),
      main = "mtcars")
  
 })
```

There is no reason why we cannot have any number of regressors.
Mathematically, we write

\[
y = \beta_0+\beta_1x_1 + \beta_2x_2+\ldots+\beta_nx_n + \epsilon
\]

where the $\beta_0,\ldots,\beta_n$ are the coefficients and
$x_1,\ldots x_n$ the independent variables.  Estimating these
coefficients is straightforward in R:

```{r}
summary(lm(mpg~wt+disp,data=mtcars))
```

In the R commands and output above, note that the "```+```" symbol
does not mean "plus"; it means "add this variable to the list of
regressors".  Thus the input line reads in English "summary of a
linear model of ```mpg``` as-a-function-of ```wt``` and ```disp```".
Looking at the output we see the estimated coefficients for weight and
displacement.  While the weight is significant, with a $p$-value of
0.00743, the coefficient for displacement has a $p$-value of 0.06362
and is not significant.